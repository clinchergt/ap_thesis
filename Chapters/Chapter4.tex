% Chapter 4

\chapter{Datos utilizados}

\label{Chapter4} % For referencing the chapter elsewhere, use \ref{Chapter1} 

``On two occasions I have been asked, "Pray, Mr. Babbage, if you put into the machine wrong figures, will the right answers come out?" ... I am not able rightly to apprehend the kind of confusion of ideas that could provoke such a question.''

\hfill ---Charles Babbage, Passages from the Life of a Philosopher

El concepto de \emph{garbage in, garbage out} (basura para adentro, basura para afuera) es muy conocido e indica como un modelo, análisis, etc. es limitado en su capacidad por los datos que se usen para alimentarlo.

En proyectos de aprendizaje artificial se dice que el 80\% del tiempo se deberá usar para la búsqueda, la limpieza y el tratado de datos. Esta proporción no se mantiene en la mayoría de casos cuando se lidia con modelos de Deep Learning pero el principio se conserva: no se debe subestimar la importancia de los datos y su integridad.


\section{Pre-procesamiento de datos}

\subsection{Datos tabulares}

\begin{table}
\centering
\begin{tabular}{c c c c | c c}
CORRELATIVO & Edad & Género & País & Sueldo & Moneda \\
\hline
1 & 24 & F & GT & 3500 & GTQ \\
2 & 145 & M & Perú & 100,500 & Soles \\
3 & 35 & N/A & .es & 1.5 & \texteuro \\
& & & & & \\
$\vdots$ &  & $\ddots$ & & & $\vdots$ \\
& & & & & \\
2304 & 18 & F & GT & 2.8k & Q \\
\end{tabular}
\label{table:tabulares}
\caption{Ejemplo de datos tabulares que alimentarán a un modelo ya sea de aprendizaje de máquina o de Deep Learning. En la tabla se ilustran algunos errores de datos (campo \emph{edad}) y datos faltantes (en el campo \emph{Género}). También se aprecian discrepancias en el formato de algunos campos (campo de \emph{Moneda} y \emph{Sueldo}) y campos completamente inservibles para un modelo (campo \emph{CORRELATIVO}.}
\end{table}

La recolección de datos para la alimentación de modelos por lo general es un proceso que involucra obtener información de distintas fuentes con distintos formatos y distintos detalles que esperar. El pre-procesamiento de datos se encarga de darle una forma congruente a los datos para que estos puedan representar el problema de forma adecuada. Este proceso puede involucrar eliminar errores de formato, identificar datos atípicos y lidiar con ellos, identificar características valiosas para un modelo, etc. Estos pasos son más comunes cuando se lidia con datos tabulares, es decir datos que están separados por categorías o columnas y deben ser tratados cada una como una variable.

\subsection{Datos textuales}

En NLP y en este trabajo se lidia con datos en forma de texto y no de forma tabular. Esto conlleva un conjunto de retos especiales a considerar; surgen conceptos nuevos a considerar. En NLP los pasos que generalmente se realizan para limpiar los datos son los siguientes:

\begin{itemize}
\item \textbf{Tokenizar.} Esta técnica de pre-procesamiento consiste en separar las cadenas de palabras --- textos --- en \emph{tokens}. Esto con el propósito de que el modelo sea capaz de digerir las cadenas de palabras por segmentos bien definidos. El delimitador de los tokens en los datos de entrada podrá ser algo simple como un caracter de espacio (' '). También existen técnicas más elaboradas, por ejemplo: \gls{tokenizacion} basado en la morfología de un lenguaje.
\item \textbf{Limitar \gls{vocabulario}.} Como consecuencia de tener cantidades grandes de datos para la alimentación del modelo se puede terminar con un vocabulario bastante numeroso. Este vocabulario está compuesto de todos los tokens únicos que se encontraron en los datos de entrada. Una técnica común para evitar utilizar tokens encontrados muy pocas veces en la data es limitar el número de tokens a utilizar. Se elige esa cantidad $n$ de datos a utilizar al ordenar los tokens por su frecuencia y tomando los $n$ más comunes.
\item \textbf{Eliminar palabras con propósito gramático.} Muchas tareas de clasificación en NLP dependen mucho del contenido de los textos. Una técnica común para poder enfatizar el valor semántico de los textos es eliminar tokens cuyo propósito es únicamente gramatical. Se podrían nombrar artículos y preposiciónes como ejemplos.
\item \textbf{Vectorizar.} Una técnica muy valiosa que consiste en representar las cadenas de caracteres en vectores en un espacio de $d$ dimensión. Mientras más grande $d$, mejores las posibilidades de capturar más el significado de cada token o palabra. En modelos del estado del arte en NLP se utiliza una dimensionalidad generalmente de $d = 300$ o $d = 400$. Algunas técnicas de vectorización son capaces de capturar mucha información semántica, al grado de poder inferir información acerca de incluso tokens no antes vistos.
\item \textbf{Zero padding.} Esta es una técnica utilizada para dar a los datos de entrada el mismo largo y permitir que el modelo no sufra por la variabilidad de longitud. Esta técnica es más común cuando se utilizan modelos de machine learning y no deep learning.
\item \textbf{Tokens especiales.} Al aplicar técnicas como limitación de vocabulario o zero padding surge la necesidad de usar tokens especiales que le indican al modelo conceptos como un término no antes visto o el final de una secuencia. Estos tokens especiales generalmente son definidos por quien realiza el proceso de tokenización aunque también se pueden incluir previo a esta etapa. Estos tokens especiales generalmente son representados en este estilo \texttt{<unk>} donde \texttt{unk} es generalmente una abreviación del concepto del token especial. 
\end{itemize}

Todos estos pasos tienen alternativas o son completamente opcionales. Para algunas tareas de clasificación de texto tareas como la eliminación de palabras gramáticas son necesarios, sin embargo para una tarea como atribución de autor de un texto es necesario incluir información que pueda identificar al autor \parencite{coulthard2004author, louwerse2004semantic}.

\section{Fuente de los datos}

LEFTy es un proyecto multi-tarea, lo cual significa que las predicciones son de distintos conceptos. Esto generalmente lleva a utilizar datos distintos para cada tarea en específico. Gracias a la ventaja que la transferencia de aprendizaje provee --- descrita en el capítulo \ref{Chapter3} --- la cantidad de datos necesaria para obtener buenos resultados no es excesiva y esto facilita la búsqueda de datos.

Las fuentes de los datos para cada subtarea son los siguientes:

\begin{itemize}
%\item \textbf{Determinación de región.} Para esta tarea se utilizaron los datasets de DSLCC \parencite{tan:2014:BUCC} --- por sus siglas en inglés Differentiate Similar Languages Corpus Collection. Estos datos incluyen extractos de artículos de periodismo etiquetados por región y variante de español.

\item \textbf{Determinación de género y edad.} El corpus de datos que se utiliza para estas dos tareas fue el mismo. El corpus es llamado \textit{\gls{spantext}} \parencite{villegas:2014:CACIC} y contiene textos escritos por personas hispanohablantes, cada uno etiquetado con su género y edad. Para estos datos se utilizan solamente textos con una longitud mínima de 100 palabras.

\item \textbf{Determinación de género y región en textos informales.} Adicional a las fuentes de datos mencionadas, se utilizan los datos de una competencia de perfilamiento de autores, en específico utilizando textos informales. La competencia es \gls{pan} en su edición del 2017 (\url{https://pan.webis.de/clef17/pan17-web/author-profiling.html}). Los datos han sido utilizados con el permiso de los organizadores de la competencia de PAN17 \parencite{rangel2017proceedings}. Estos datos no han sido pre-procesados de ninguna forma y la única garantía que se tiene es que hay una cantidad mínima de tuits por autor y una cantidad mínima de autores por género y región. El conjunto de datos se encuentra dividido entre conjuntos de datos para entrenamiento y para prueba de desempeño.

\item \textbf{Datos de pre-entrenamiento.} Para la fase de pre-entrenamiento del modelo de lenguaje se utilizan datos de \textit{Wikipedia}. Se descarga un \textit{dump} --- colección de datos --- de noviembre del 2018. De estos datos se filtran entonces los textos que tenían como mínimo $m = 1000$ cantidad de palabras. Luego de esto, se seleccionaron los primeros 100 millones de \textit{tokens}, esto para simular una distribución de artículos similar a \textit{WikiText-103} \parencite{merity2016pointer} la cual fue propuesta como una muestra más realista de la distribución de textos de \textit{Wikipedia}.
\end{itemize}
\begin{comment}
\section{Distribución de los datos}

En esta sección se expondrá un análisis exploratorio de los datos. Esto con el fin de poder apreciar con qué información se contará, también ayudará a poder tomar decisiones informadas más adelante acerca de los modelos a utilizar. Este tipo de análisis es superficial y deberá mostrar únicamente características principales de los datos y sus distribuciones.

\subsection{Textos informales (PAN'17)}

\subsection{Textos formales (SpanText)}

seguir aca con detalles de datos si es necesario
graficas de los datos

mover a apéndice?
\end{comment}



