% Chapter 6

\chapter{Conclusiones}

\label{Chapter6} % For referencing the chapter elsewhere, use \ref{Chapter6}

En este capítulo se presentan las conclusiones obtenidas a partir del trabajo de investigación y los resultados encontrados.

\section{Conclusiones}

El campo de aprendizaje profundo ha mostrado mucha promesa en los últimos años, pero el área de NLP no ha mostrado tanta promesa sino hasta el último año. En este trabajo se busca fomentar y profundizar más la penetración que han tenido estas técnicas en el área tanto en sectores industriales como académicos.

Para poder plantear las conclusiones de este trabajo se deberá recordar los objetivos principales mencionados en un comienzo. Se pretendía plantear un sistema competitivo a niveles del estado del arte que no requiriera ingeniería de características sobre los textos; un modelo que no requiriera expertiz en el campo de la lingüística para poder ser diseñado; un modelo que no necesitara tantos ejemplos de entrenamiento para poder generalizar y desempeñarse bien la tarea asignada. LEFTy cumple con estos objetivos de una forma elegante y novedosa aplicando \textit{transfer learning} a la tarea de perfilamiento de autores en el área de NLP.

Este concepto permite abrir muchas puertas y se ha demostrado que esto no sólo aplica para el área de visión artificial con muchas publicaciones en el último año de laboratorios grandes como OpenAI, fastai, Google, Deep Mind, etc. La promesa que trae esta técnica a NLP permitiría aplicarse a una infinidad de tareas sin depender de muchos ejemplos, justo como se pudo apreciar con los resultados presentados sobre textos formales.

Sin embargo, \textit{transfer learning} tiene compromisos que se deben tomar en cuenta a pesar de todas las ventajas que provee. Una de ellas es no poder cambiar la estructura del modelo pre-entrenado de una manera muy significativa sin el riesgo de una pérdida catastrófica del aprendizaje general con el que se cuenta. Lo cual lleva también a la conclusión que se debe tener mucho cuidado al afinar un modelo general a una tarea específica en NLP ya que se tiene el mismo riesgo si se entrena de manera desenfrenada.

En este trabajo se reiteran algunas técnicas de afinamiento de modelos, tanto para modelos de lenguaje como para modelos clasificadores de una tarea final, reproduciendo de manera parcial los resultados presentados por \citeauthor{howard2018}.

Los resultados presentados en el capítulo \ref{Chapter5} de este trabajo compiten con los modelos del estado del arte y con gente que se ha dedicado por años al estudio de NLP, lingüística y su intersección como ciencias. Esto demuestra que no se debe ser persona experta para lograr aprovechar el poder del deep learning.

Se presentó un modelo de lenguaje del idioma español el cual puede ser utilizado para una cantidad ilimitada de tareas y se hizo público para que pueda ser aprovechado por la comunidad científica en esta área; este ha sido descargado más de 30 veces al momento de la publicación de este trabajo.

El modelo publicado también provee la ventaja de permitir una reducción de costos al entrenar modelos clasificadores futuros, ya que el mayor costo presentado en este trabajo fue el del entrenamiento del modelo de lenguaje general entrenado sobre datos de Wikipedia. Este hecho promete que el uso de aprendizaje profundo no sea prohibitivo para compañías e individuos que no tienen acceso a poder computacional masivo o incluso ilimitado como algunas compañías, lo cual se considera una contribución muy valiosa a la comunidad.

\section{Trabajo futuro}

Las estrategias, técnicas y tecnologías utilizadas en este trabajo muestra un balance entre obtener resultados del estado del arte y realizar la tarea sin sobrellevar costos prohibitivos. Debido a esto, algunas técnicas no fueron utilizadas debido a limitaciones económicas o temporales y una sección de trabajo futuro es necesaria.

Asimismo, debido a que el desarrollo en esta área tiene un ritmo muy acelerado, durante la elaboración de este trabajo se realizaron múltiples publicaciones con nuevos aportes a la comunidad científica que pueden ser de mucho valor para la aplicación específica descrita en este proyecto.

A continuación se plantea el trabajo futuro a considerar:

\begin{itemize}

\item La adición más simple que se podría hacer con el trabajo realizado es utilizar LSTMs bidireccionales. En este caso en particular significaría entrenar un modelo que evalúe los tuis con el orden inverso de las palabras y realizar un ensamble de los modelos resultantes.

\item \textcite{radford2019language} han mostrado que utilizar textos más diversos y en mayores cantidades para el entrenamiento de modelos de lenguaje puede resultar en una ventaja adicional, no sólo en ese modelo en particular sino en las tareas específicas para las que después se utilizará el sistema pre-entrenado.

\item El uso de una nueva estructura llamada transformador ha tomado un auge en el año 2018 y 2019 y ha presentado nuevos resultados del estado del arte. En algunas ocasiones ha superado por un margen elevado al resultado a vencer. Esto indica que puede aportar mucho valor usar una estructura de trasnformador en lugar de una de LSTM. Los transformadores proveen ventajas como la posibilidad de ser entrenados en modalidades paralelas. Sin embargo, muchos resultados han necesitado de mucho poder computacional por lo que puede llegar a ser prohibitivo su uso.

\item En este trabajo se mostró que el pre-procesamiento de los textos antes de ser alimentados al modelo no es perfecto, por lo que se deben explorar formas de refinar esto. En particular la normalización y conservación del aspecto semántico de los textos, especialmente los informales, requerirán de mayor esfuerzo y experimentación.

\item Para este trabajo se clasificaron los tuits individualmente, sin embargo existía la posibilidad de utilizar concatenación de textos y evaluar sobre el texto resultante para cada autor. Se podría usar este abordamiento al problema y evaluar si se obtiene mejor desempeño. Esto tendría el propósito de mejorar la posición en una competencia y no de avanzar el campo en esta tarea en específico.

\item Muchos avances se han realizado en los últimos años con respecto a modelos multi-tarea (multitask) --- los cuales son modelos que tienen la capacidad de realizar múltiples predicciones, cada una de una tarea independiente. Tener solamente un modelo resultaría en menores tiempos de inferencia y menores requerimientos de memoria en el servidor donde se monte el sistema de predicción.


\end{itemize}

